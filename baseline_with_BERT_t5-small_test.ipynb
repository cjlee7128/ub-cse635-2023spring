{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_workers = 0 \n",
    "device = 'cuda:0' \n",
    "max_input_length = 64 \n",
    "bert_max_input_length = 512 \n",
    "bert_model = 'distilbert-base-uncased' \n",
    "model_name = \"./_baseline_with_BERT_t5-small/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple \n",
    "import json \n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from transformers  import AutoTokenizer, AutoModelForSeq2SeqLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import multilabel_confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "import evaluate \n",
    "from bert_score import score as b_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithDial_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, contexts, answers, tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.questions = questions \n",
    "        self.contexts = contexts \n",
    "        self.answers = answers \n",
    "\n",
    "        # https://github.com/nunziati/bert-vs-t5-for-question-answering/blob/main/MyDataset.py \n",
    "        if len(self.questions) != len(self.contexts): \n",
    "            raise Exception(\n",
    "                \"something wrong while building the dataset: questions and contexts in different dimensions\") \n",
    "        if len(self.questions) != len(self.answers):\n",
    "            raise Exception(\n",
    "                \"something wrong while building the dataset: questions and answers result in different dimensions\")\n",
    "\n",
    "        self.item_count: int = len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.questions[index], self.contexts[index], self.answers[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.item_count \n",
    "\n",
    "    def pack_minibatch(self, data: List[Tuple[str, str, str]]):\n",
    "        \"\"\"Pack mini-batch function\n",
    "        \"\"\"\n",
    "        return zip(*data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data_path, hal_data_path): \n",
    "    question_list = ['Is the response hallucinated?', 'What are the response attribution classes?', 'What are the speech acts?', 'What is the faithful response to this?'] \n",
    "\n",
    "    questions = [] \n",
    "    questions_index = [] \n",
    "    contexts = [] \n",
    "    answers = [] \n",
    "\n",
    "    with open(data_path, 'r') as f1, open(hal_data_path, 'r') as f2: \n",
    "        data = json.load(f1) \n",
    "        hal_data = json.load(f2) \n",
    "\n",
    "    for conversation, hal_conversation in zip(data, hal_data): \n",
    "        for turn, hal_turn in zip(conversation['utterances'], hal_conversation['utterances']): \n",
    "\n",
    "            # Task1 \n",
    "            # question, history, knowledge, response -> \"Yes\" or \"No\" \n",
    "            # knowledge and response are usually short \n",
    "            questions.append(f\"question: {question_list[0]} response: {turn['response']}\")\n",
    "            contexts.append(f\"{turn['knowledge']} {', '.join(turn['history'])}\")\n",
    "            answers.append(\"No\") \n",
    "            questions_index.append(0) \n",
    "\n",
    "            hal_turn['history'] = ['null' if h is None else h for h in hal_turn['history']]\n",
    "            hal_turn['response'] = 'null' if hal_turn['response'] is None else hal_turn['response'] \n",
    "            # knowledge and response are usually short \n",
    "            questions.append(f\"question: {question_list[0]} response: {hal_turn['response']}\") \n",
    "            contexts.append(f\"{hal_turn['knowledge']} {', '.join(hal_turn['history'])}\") \n",
    "            answers.append(\"Yes\")\n",
    "            questions_index.append(0) \n",
    "\n",
    "            # Task2-1 \n",
    "            # question, history, knowledge, response -> BEGIN tag(s) \n",
    "            # knowledge and response are usually short \n",
    "            questions.append(f\"question: {question_list[1]} response: {hal_turn['response']}\") \n",
    "            contexts.append(f\"{hal_turn['knowledge']} {', '.join(hal_turn['history'])}\") \n",
    "            answers.append(f\"{', '.join(hal_turn['BEGIN'])}\") \n",
    "            questions_index.append(1) \n",
    "\n",
    "            # Task2-2 \n",
    "            # question, history, knowledge -> VRM tag(s) \n",
    "            if hal_turn['VRM'][0] != '': \n",
    "                # knowledge is usually short \n",
    "                questions.append(f\"question: {question_list[2]}\") \n",
    "                contexts.append(f\"{hal_turn['knowledge']} {', '.join(hal_turn['history'])}\") \n",
    "                answers.append(f\"{', '.join(['Acknowledgment' if v=='Ack.' else v for v in hal_turn['VRM']])}\") \n",
    "\n",
    "                questions_index.append(2) \n",
    "\n",
    "            # Task3 \n",
    "            # question, history, knowledge -> response \n",
    "            # knowledge is usually short \n",
    "            questions.append(f\"question: {question_list[3]}\") \n",
    "            contexts.append(f\"{turn['knowledge']} {', '.join(turn['history'])}\") \n",
    "            answers.append(f\"{turn['response']}\") \n",
    "            questions_index.append(3) \n",
    "\n",
    "    return questions, contexts, answers, questions_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions, test_contexts, test_answers, test_questions_index = build_data('data/test.json', 'data/hal_test.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/distilbert-base-uncased \n",
    "from transformers import DistilBertModel \n",
    "# https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model \n",
    "class CustomDistilBertModel(torch.nn.Module): \n",
    "    def __init__(self, bert_model_name, t5_seq_len, t5_emb_dim): \n",
    "        super(CustomDistilBertModel, self).__init__() \n",
    "        self.t5_seq_len = t5_seq_len \n",
    "        self.t5_emb_dim = t5_emb_dim \n",
    "        self.db_model = DistilBertModel.from_pretrained(bert_model_name) \n",
    "        # (B, bert_seq_len, 768) -> (B, t5_seq_len, t5_emb_dim) \n",
    "        self.linear1 = torch.nn.Linear(768, t5_emb_dim) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        # (B, bert_seq_len, 768) \n",
    "        db_outputs = self.db_model(input_ids, attention_mask=attention_mask).last_hidden_state \n",
    "        # (B, t5_seq_len, t5_emb_dim) \n",
    "        if db_outputs.size(1) < self.t5_seq_len: \n",
    "            diff = self.t5_seq_len - db_outputs.size(1) \n",
    "            db_outputs = torch.cat((db_outputs, db_outputs[:, -1:, :].repeat(1, diff, 1)), 1) \n",
    "        linear1_output = self.linear1(db_outputs[:, :self.t5_seq_len, :]) \n",
    "\n",
    "        return linear1_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(model_name + 't5-small/tokenizer/best-f1')\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(model_name + 't5-small/model/best-f1')  \n",
    "\n",
    "db_tokenizer = AutoTokenizer.from_pretrained(model_name + bert_model + '/tokenizer/best-f1') \n",
    "\n",
    "db_model = torch.load(f'{model_name}{bert_model}/model/best-f1/pytorch_model.pt') \n",
    "\n",
    "test_set = FaithDial_Dataset(test_questions, test_contexts, test_answers, t5_tokenizer) \n",
    "\n",
    "my_test_dataloader = DataLoader(test_set, batch_size=batch_size,\n",
    "                                num_workers=num_workers, collate_fn=lambda data_: test_set.pack_minibatch(data_))\n",
    "\n",
    "t5_model.to(device)\n",
    "db_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model.eval() \n",
    "db_model.eval() \n",
    "with torch.no_grad():\n",
    "    model_predictions_encoded = [] \n",
    "    for questions, contexts, _ in tqdm(my_test_dataloader): \n",
    "        db_inputs = contexts \n",
    "        db_encoded_inputs = db_tokenizer(db_inputs, \n",
    "                                        padding=\"longest\", \n",
    "                                        max_length=bert_max_input_length, \n",
    "                                        truncation=True, \n",
    "                                        return_tensors=\"pt\").to(device)\n",
    "        db_outputs = db_model(**db_encoded_inputs) \n",
    "\n",
    "        inputs = questions \n",
    "        encoded_inputs = t5_tokenizer(inputs,\n",
    "                                      # padding=\"longest\", \n",
    "                                      padding='max_length', \n",
    "                                      max_length=max_input_length, \n",
    "                                      truncation=True,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      )\n",
    "        \n",
    "        encoded_inputs, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask \n",
    "\n",
    "        encoded_inputs = encoded_inputs.to(device) \n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        t5_encoder = t5_model.get_encoder() \n",
    "\n",
    "        t5_encoder_outputs = t5_encoder(input_ids=encoded_inputs,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        ) \n",
    "        t5_encoder_outputs['last_hidden_state'] = (t5_encoder_outputs['last_hidden_state'] + db_outputs) / 2 \n",
    "\n",
    "        model_predictions = t5_model.generate(input_ids=encoded_inputs,\n",
    "                                              attention_mask=attention_mask,\n",
    "                                              encoder_outputs=t5_encoder_outputs, \n",
    "                                              )\n",
    "        model_predictions_encoded += model_predictions.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = t5_tokenizer.batch_decode(model_predictions_encoded, skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_LIST = ['Uncooperative', 'Hallucination', 'Entailment', 'Generic'] \n",
    "VRM_LIST = ['Disclosure', 'Acknowledgment', 'Edification', 'Advisement', 'Question']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_dict = dict(zip(BEGIN_LIST, range(len(BEGIN_LIST)))) \n",
    "VRM_dict = dict(zip(VRM_LIST, range(len(VRM_LIST)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/10018679/python-find-closest-string-from-a-list-to-another-string \n",
    "# https://docs.python.org/3/library/difflib.html#difflib.get_close_matches \n",
    "import difflib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_pred = [] \n",
    "task1_true = [] \n",
    "\n",
    "task2_BEGIN_pred = [] \n",
    "task2_BEGIN_true = [] \n",
    "\n",
    "task2_VRM_pred = [] \n",
    "task2_VRM_true = [] \n",
    "\n",
    "task3_pred = [] \n",
    "task3_true = [] \n",
    "\n",
    "for i, (pred, true) in enumerate(zip(model_predictions, test_answers)): \n",
    "    task_index = test_questions_index[i] \n",
    "    if task_index == 0:     # Task1 \n",
    "        task1_true.append(0 if true=='No' else 1) \n",
    "        if pred == 'No': \n",
    "            task1_pred.append(0) \n",
    "        elif pred == 'Yes': \n",
    "            task1_pred.append(1) \n",
    "        else: \n",
    "            # raise RuntimeError(f'Task1 prediction format wrong {i} index value: {pred}')\n",
    "            # https://stackoverflow.com/questions/10018679/python-find-closest-string-from-a-list-to-another-string \n",
    "            # https://docs.python.org/3/library/difflib.html#difflib.get_close_matches \n",
    "            _pred = difflib.get_close_matches(pred, ['Yes', 'No'], n=1, cutoff=0.3) \n",
    "            if _pred == 'No': \n",
    "                task1_pred.append(0) \n",
    "            else: \n",
    "                task1_pred.append(1)\n",
    "        # pass \n",
    "    elif task_index == 1:   # Task2 BEGIN \n",
    "        task2_BEGIN_true.append([BEGIN_dict[t] for t in true.split(', ')]) \n",
    "        task2_BEGIN_pred.append([BEGIN_dict[p] if p in BEGIN_LIST else difflib.get_close_matches(p, BEGIN_LIST, n=1, cutoff=0.0)[0] for p in pred.split(', ')]) \n",
    "        # pass \n",
    "    elif task_index == 2:   # Task2 VRM \n",
    "        task2_VRM_true.append([VRM_dict[t] for t in true.split(', ')]) \n",
    "        task2_VRM_pred.append([VRM_dict[p] if p in VRM_LIST else difflib.get_close_matches(p, VRM_LIST, n=1, cutoff=0.0)[0] for p in pred.split(', ')]) \n",
    "    else:                   # Task3 Response \n",
    "        task3_true.append(true) \n",
    "        task3_pred.append(pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.json', 'r') as r_f, open('data/test_baseline_with_BERT_t5-small_predicted.json', 'w') as w_f: \n",
    "    data = json.load(r_f) \n",
    "    i = 0 \n",
    "    for conversation in data: \n",
    "        for turn in conversation['utterances']: \n",
    "            turn['predicted_response'] = task3_pred[i] \n",
    "            i = i + 1 \n",
    "            \n",
    "    assert i == len(task3_pred) \n",
    "\n",
    "    json_object = json.dumps(data, indent=2) \n",
    "    w_f.write(json_object) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(task1_true, task1_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(task1_true, task1_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task2_BEGIN_true = MultiLabelBinarizer(classes=range(len(BEGIN_LIST))).fit_transform(task2_BEGIN_true) \n",
    "_task2_BEGIN_pred = MultiLabelBinarizer(classes=range(len(BEGIN_LIST))).fit_transform(task2_BEGIN_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(_task2_BEGIN_true, _task2_BEGIN_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(_task2_BEGIN_true, _task2_BEGIN_pred, average='macro')) \n",
    "print(BEGIN_LIST) \n",
    "print(multilabel_confusion_matrix(_task2_BEGIN_true, _task2_BEGIN_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task2_VRM_true = MultiLabelBinarizer(classes=range(len(VRM_LIST))).fit_transform(task2_VRM_true)\n",
    "_task2_VRM_pred = MultiLabelBinarizer(classes=range(len(VRM_LIST))).fit_transform(task2_VRM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(_task2_VRM_true, _task2_VRM_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(_task2_VRM_true, _task2_VRM_pred, average='macro'))\n",
    "print(VRM_LIST) \n",
    "print(multilabel_confusion_matrix(_task2_VRM_true, _task2_VRM_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\") \n",
    "results = bleu.compute(predictions=task3_pred, references=task3_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge') # pip install rouge-score \n",
    "results = rouge.compute(predictions=task3_pred, references=task3_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = b_score(task3_pred, task3_true, lang='en', verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_task1_data_with_pred(data_path, task3_pred): \n",
    "    questions = [] \n",
    "    contexts = [] \n",
    "\n",
    "    with open(data_path, 'r') as f1: \n",
    "        data = json.load(f1) \n",
    "\n",
    "    i = 0 \n",
    "    for conversation in data: \n",
    "        for turn in conversation['utterances']: \n",
    "            # questions.append(f\"question: Is the response hallucinated? knowledge: {turn['knowledge']} response: {task3_pred[i]} history: {', '.join(turn['history'])}\")\n",
    "            questions.append(f\"question: Is the response hallucinated? response: {task3_pred[i]}\")\n",
    "            contexts.append(f\"{turn['knowledge']} {', '.join(turn['history'])}\")\n",
    "            i = i + 1 \n",
    "\n",
    "    assert i == len(task3_pred)\n",
    "\n",
    "    return questions, contexts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_pred_task1_data = build_task1_data_with_pred('data/test.json', task3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithDial_test_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, contexts, tokenizer):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.questions = questions \n",
    "        self.contexts = contexts \n",
    "\n",
    "        self.item_count: int = len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.questions[index], self.contexts[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.item_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_t3_t1 = FaithDial_test_Dataset(task3_pred_task1_data[0], task3_pred_task1_data[1], t5_tokenizer) \n",
    "\n",
    "my_test_t3_t1_dataloader = DataLoader(test_set_t3_t1, batch_size=batch_size,\n",
    "                                        num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model.eval() \n",
    "db_model.eval() \n",
    "with torch.no_grad():\n",
    "    model_predictions_encoded_t3_t1 = []\n",
    "    for questions, contexts in tqdm(my_test_t3_t1_dataloader): \n",
    "        db_inputs = contexts \n",
    "        db_encoded_inputs = db_tokenizer(db_inputs, \n",
    "                                        padding=\"longest\", \n",
    "                                        max_length=bert_max_input_length, \n",
    "                                        truncation=True, \n",
    "                                        return_tensors=\"pt\").to(device)\n",
    "        db_outputs = db_model(**db_encoded_inputs) \n",
    "\n",
    "        inputs = questions \n",
    "        encoded_inputs = t5_tokenizer(inputs,\n",
    "                                      # padding=\"longest\", \n",
    "                                      padding='max_length', \n",
    "                                      max_length=max_input_length, \n",
    "                                      truncation=True,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      )\n",
    "        \n",
    "        encoded_inputs, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask \n",
    "\n",
    "        encoded_inputs = encoded_inputs.to(device) \n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        t5_encoder = t5_model.get_encoder() \n",
    "\n",
    "        t5_encoder_outputs = t5_encoder(input_ids=encoded_inputs,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        ) \n",
    "        t5_encoder_outputs['last_hidden_state'] = (t5_encoder_outputs['last_hidden_state'] + db_outputs) / 2 \n",
    "\n",
    "        model_predictions = t5_model.generate(input_ids=encoded_inputs,\n",
    "                                              attention_mask=attention_mask,\n",
    "                                              encoder_outputs=t5_encoder_outputs, \n",
    "                                              )\n",
    "        model_predictions_encoded_t3_t1 += model_predictions.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_t3_t1 = t5_tokenizer.batch_decode(model_predictions_encoded_t3_t1, skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_yes = 0 \n",
    "for pred in model_predictions_t3_t1: \n",
    "    if pred == 'Yes': \n",
    "        num_of_yes = num_of_yes + 1 \n",
    "    elif pred == 'No': \n",
    "        pass \n",
    "    else: \n",
    "        raise RuntimeError('Wrong prediction') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_of_yes / len(model_predictions_t3_t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d50cec98d87c52ffb059a302fec9b3cc275ef384a9a9b3a5ff0b8f6496382eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
